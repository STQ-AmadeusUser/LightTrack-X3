# LightTrack on Horizon X3 pi

This is an unofficial X3 pi deployment of LightTrack[1] based on Python, whose official verision is as follows:  
[Official implementation of LightTrack](https://github.com/researchmm/LightTrack)

## Model Overview
Tracker | MACs | Params | FPS | Avg Latency | DDR Latency | Subgraph | BPU Util1 | BPU Util2 | DTB70 Success
--- | --- | --- | --- |--- |--- |--- |--- |--- |---
LightTrack | 0.53G | 1.97M | 33.48 | 118.72ms | 239.43ms | 3 | 20.36% | 14.18% | 0.587

We test BPU utilization rate by using hrut_somstatus while testing static performance with 4 threads:
```
hrut_somstatus -n 10000 -d 1
```
We record the outputs of hrut_somstatus, delete the values which are lower than 10%
and compute the average of BPU utilization rate.  
Note that in order to ensure fairness when compared with other trackers,
we do not execute any optimization of operators when deploying model on Horizon X3 pi.

## Demos for ChasingDrones
```
cd video
tar zxvf ChasingDrones.tar.gz
```
<div align="center">
  <img src="https://github.com/STQ-AmadeusUser/LightTrack-X3/blob/main/images/ChasingDrones_result.gif">
</div>
The GIF is generated by using moviepy to covert ChasingDrones_result.mp4, which the demo on X3 pi outputs:

```
from moviepy.editor import *
clip = (VideoFileClip("ChasingDrones_result.mp4").resize((640, 360)))
clip.write_gif("ChasingDrones_result.gif", fps=15)
```

## A Demo on X3 pi
You can execute running_on_X3.py to experience LightTrack on ChasingDrones, one video in DTB70[2] benchmark:
```
cd tracking
python running_on_X3.py
```
The code will generate a video named ChasingDrones_result.mp4 in "video" directory.

## A Demo on Laptop
We also provide demo.py for laptop running:
```
cd tracking
python demo.py --cfg ../experiments/LightTrack.yaml --video *.mp4
```
You can change the arg "video" to whatever video you want to test. 
If default, the code will open your laptop camera.

## How to Deploy (for developers)
We have converted the network of LightTrack to LightTrack.bin, a file with X3-pi-specialized format, in "deploy" directory.  
If you want to generate the file by yourself, you can take advantage of our code and [follow this instruction](https://developer.horizon.cc/documents_rdk/category/toolchain_development).
1. Get the onnx model: LightTrack.onnx
```
cd deploy
python convert_onnx.py
```
You can visualize onnx model by using [netron](https://netron.app/).

2. Simplify onnx model: LightTrack_sim.onnx
```
python -m onnxsim LightTrack.onnx LightTrack_sim.onnx
```
3. Generate files for calibration:

This step requires training datasets described in official implementation of LightTrack or pysot[3].  
We directly provide calibration files in calibration.tar.gz.

4. Convert onnx model: LightTrack.bin
```
hb_mapper makertbin --config LightTrack.yaml --model-type onnx
```
This step needs Horizon AI Tool-chain. We create an anaconda environment for python packages
as mentioned in [here](https://developer.horizon.cc/documents_rdk/toolchain_development/beginner).
Note that we do not use docker image. HiFT.yaml contains all parameters used in conversion.

5. Log file is "deploy/hb_mapper_makertbin.log".
6. Other generated files are in "deploy/model" directory.

## Static Performance
We provide static performance of LightTrack by running on X3 pi:
1. 1 thread:
```
hrt_model_exec perf --model_file LightTrack.bin --thread_num 1
```
<div align="center">
  <img src="https://github.com/STQ-AmadeusUser/LightTrack-X3/blob/main/images/1_thread.png">
</div>

2. 4 thread:
```
hrt_model_exec perf --model_file LightTrack.bin --thread_num 4
```
<div align="center">
  <img src="https://github.com/STQ-AmadeusUser/LightTrack-X3/blob/main/images/4_thread.png">
</div>

## Version
1. System on X3 pi: [ubuntu-preinstalled-server-arm64.img.xz](http://sunrise.horizon.cc/downloads/os_images/2.0.0/release/)
   (2.0.0)
2. Horizon AI Tool-chain:
    ```
    wget -c ftp://xj3ftp@vrftp.horizon.ai/ai_toolchain/ai_toolchain.tar.gz --ftp-password=xj3ftp@123$%
    ```
    1. hbdk-3.48.6-cp38-cp38-linux_x86_64.whl
    2. hbdk_model_verifier-3.48.6-py3-none-linux_x86_64.whl
    3. horizon_nn-0.21.2-cp38-cp38-linux_x86_64.whl
    4. horizon_tc_ui-1.21.6-cp38-cp38-linux_x86_64.whl
3. [OE package](https://developer.horizon.ai/forumDetail/136488103547258769): horizon_xj3_open_explorer_v2.6.2b-py38_20230606
    ```
    wget -c ftp://vrftp.horizon.ai/Open_Explorer_gcc_9.3.0/2.6.2b/horizon_xj3_open_explorer_v2.6.2b-py38_20230606.tar.gz
    ```
4. PyTorch:
    1. for convert_onnx.py: torch-1.9.0+cu111-cp37-cp37m-linux_x86_64.whl
    2. for demo on laptop: torch-1.9.0+cu111-cp37-cp37m-linux_x86_64.whl
    
## Reference
[1] LightTrack: Finding Lightweight Neural Networks for Object Tracking via One-Shot Architecture Search  
[2] DTB70: https://github.com/flyers/drone-tracking  
[3] PySOT: https://github.com/STVIR/pysot